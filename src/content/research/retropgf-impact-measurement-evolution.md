---
id: '3'
slug: retropgf-impact-measurement-evolution
name: 'Impact Measurement in Retroactive Funding: Evolution Through RetroPGF 3-6'
shortDescription: How Optimism has evolved its impact measurement approaches across four RetroPGF rounds, with lessons for the broader ecosystem.
tags:
  - impact measurement
  - retroactive funding
  - evaluation
  - optimism
lastUpdated: '2024-12-25'
relatedMechanisms:
  - retroactive-funding
  - attestation-based
relatedApps:
  - optimism-retropgf
banner: /content-images/research/retropgf-impact-measurement-evolution/banner.png
---

**Type:** Analysis
**Authors:** Gitcoin Research

**Sources:**
- [Optimism RetroPGF](https://retrofunding.optimism.io)
- [OP Governance Forum](https://gov.optimism.io)

## The Measurement Challenge

Retroactive funding promises to reward demonstrated impact, but measuring "impact" for public goods is notoriously difficult. Optimism has iterated significantly across rounds.

## Evolution Across Rounds

### RetroPGF 3 (January 2024)

30M OP to 501 projects
- **Approach**: Badgeholder voting with minimal structure
- **Categories**: OP Stack, Governance, Dev Ecosystem, End Users
- **Challenges**:
  - 644 projects too many to evaluate thoroughly
  - Cross-category comparison difficult
  - Some gaming of profile presentation

### RetroPGF 4 (June 2024)

10M OP with focused scope
- **Approach**: Narrowed to specific impact areas
- **Improvements**: Better category definition, clearer criteria
- **Results**: More consistent evaluation, still some subjectivity

### RetroPGF 5 (Fall 2024)

8M OP with refined process
- **Focus**: Dev tooling and infrastructure
- **Innovations**:
  - Impact metrics framework
  - Badgeholder training
  - Clearer evaluation rubrics

### RetroPGF 6 (Active)

2.4M OP focused on governance
- **Scope**: Governance contributions only
- **Approach**: Narrow focus allows depth
- **New**: Algorithmic initial ranking

## Key Learnings

1. **Scope matters**: Narrower scope enables better evaluation
2. **Training helps**: Badgeholder preparation improves consistency
3. **Metrics + judgment**: Neither purely quantitative nor qualitative works alone
4. **Iteration required**: Each round informs the next

## Recommendations for Other Programs

1. Start with narrow scope and expand
2. Invest in evaluator training and support
3. Build impact measurement infrastructure
4. Plan for multi-round iteration